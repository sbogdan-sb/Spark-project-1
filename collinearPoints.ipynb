{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 1: Collinear Points\n",
    "\n",
    "For this programming assignment, we'll be using a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba30def68618f26e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f0aaa1b0eeb62ff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Collinear points\n",
    "\n",
    "Definition of collinearity[1]: In geometry, collinearity of a set of points is the property of their lying on a single line. A set of points with this property is said to be collinear.\n",
    "\n",
    "![](non-collinear-points.jpg)\n",
    "\n",
    "Here, points P,Q,R and A,R,B are collinear. However, points A,B,C are non-collinear. For more, refer [2].\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/Collinearity\n",
    "2. http://www.mathcaptain.com/geometry/collinear-points.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-888539157720ea6e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Parameterizing lines\n",
    "In order to determine whether a set of points all lie on the same line we need a standard way to define (or parametrize) a line.\n",
    "\n",
    "* One way of defining a line is as the set of points $(x,y)$ such that $y=ax+b$ for some fixed real values $a,b$.\n",
    "* We call $a$ the **slope** of the line and $b$ is the $y$-intercept which is defined as the value of $y$ when $x=0$.\n",
    "* This parameterization works for *almost* all lines. It does not work for vertical lines. For those lines we define $a$ to be **infinity** and $b$ to be the $x$ intercept of the line (the line is parallel to the $y$ axis so it does not intercept the $y$ axis (other than if it is the vertical line going through the origin).\n",
    "\n",
    "To summarize, given two different points $(x_1,y_1) \\neq (x_2,y_2)$, we define the parameterization $(a,b)$ as:\n",
    "* **if $x_1=x_2$: ** $(\\mbox{Inf},x_1)$ \n",
    "* **Else:** $(a,b)$ such that $y_1=a x_1 +b$ and $y_2=a x_2 +b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2b044c997c9d772",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task\n",
    "\n",
    "Given an input file with an arbitrary set of co-ordinates, your task is to use pyspark library functions and write a program in python3 to find if three or more points are collinear.\n",
    "\n",
    "For instance, if given these points: {(1,1), (0,1), (2,2), (3,3), (0,5), (3,4), (5,6), (0,-3), (-2,-2)}\n",
    "\n",
    "Sets of collinear points are: {((-2,-2), (1,1), (2,2), (3,3)), ((0,1), (3,4), (5,6)), ((0,-3), (0,1), (0,5))}. Note that the ordering of the points in a set or the order of the sets does not matter. \n",
    "\n",
    "Note: \n",
    "<ul>\n",
    "  <li>Every set of collinear points has to have <b>at least three points</b> (any pair of points lie on a line).</li>\n",
    "  <li>There are two types of test cases:\n",
    "      <ul>\n",
    "      <li><b>Visible Test cases</b>: Test cases given to you as a part of the notebook. These tests will help you validate your program and figure out bugs in it if any.</li>\n",
    "      <li><b>Hidden Test cases</b>: Test cases that are not given as a part of the notebook, but will be used for grading. <br>Cells in this notebook that have \"<i>##Hidden test cases here</i>\" are read-only cells containing hidden tests.</li>\n",
    "      </ul>\n",
    "  </li>\n",
    "  <li>Any cell that does not require you to submit code cannot be modified. For example: Assert statement unit test cells. Cells that have \"**# YOUR CODE HERE**\" are the ONLY ones you will need to alter. </li>\n",
    "  <li>DO NOT change the names of functions. </li>\n",
    "      \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-311d3abd841773a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Description of the Approach\n",
    "\n",
    "The goal of this assignment is to make you familiar with programming using pyspark. There are many ways to find sets of collinear points from a list of points. For the purposes of this assignment, we shall stick with the below approach:\n",
    "\n",
    "1. List all pairs of points. You can do that efficiently in spark by computing cartesian product of the list of points with itself. For example, given three points $[(1,0), (2,0), (3,0)]$, we construct a list of nine pairs  \n",
    "$[((1,0),(1,0)),((1,0), (2,0)),((1,0),(3,0))$  \n",
    "$((2,0),(1,0)),((2,0), (2,0)),((2,0),(3,0))$  \n",
    "$((3,0),(1,0)),((3,0), (2,0)),((3,0),(3,0))]$  \n",
    "\n",
    "2. Remove the pairs in which the same point appears twice such as $((2,0),(2,0))$. After these elimination you end up (for this example) with a list of just six pairs:  \n",
    "$[((1,0),(2,0)),((1,0),(3,0)),((2,0),(1,0)),((2,0),(3,0)),((3,0),(1,0)),((3,0),(2,0))]$\n",
    "\n",
    "2. For each pair of points, find the parameterization $(a,b)$ of the line connecting them as described above.\n",
    "\n",
    "3. Group the pairs according to their parameters. Clearly, if two pairs have the same $(a,b)$ values, all points in the two pairs lie on the same line.\n",
    "\n",
    "3. Eliminate the groups that contain only one pair (any pair of points defines a line).\n",
    "4. In each of the remaining groups, unpack the point-pairs to identify the individual points.\n",
    "Note that if a set of points $(x_1,y_1),\\ldots,(x_k,y_k)$ lie on the same line then each point will appear $k-1$ times in the list of point-pairs. You therefore need to transform the list of points into sets to remove duplicates.\n",
    "\n",
    "5. Output the sets of 3 or more colinear points.\n",
    "\n",
    "Your task is to implement the described algorithm in Spark. You should use RDD's all the way through and collect the results into the driver only at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8160704dcdb034d0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "hide_input": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f266efcf9bf6846",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "#We can create a SparkConf() object and use it to initialize the spark context\n",
    "conf = SparkConf().setAppName(\"Collinear Points\").setMaster(\"local[4]\") #Initialize spark context using 4 local cores as workers\n",
    "sc = SparkContext(conf=conf)    \n",
    "\n",
    "from pyspark.rdd import RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-773a47aa41687e4f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Helper Functions\n",
    "Here are some helper functions that you are encouraged to use in your implementations. Do not change these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49ad662071f3985a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The function <font color=\"blue\">format_result</font> takes an element of the form shown below in the example. It outputs a tuple of all points that are collinear (shown below).\n",
    "\n",
    "Input: ((A,slope), [C1,..., Ck]) where each of A, C1, ..., Ck is a point of form (Ax, Ay) and slope is of type float.\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Code</font>**\n",
    "``` python\n",
    "my_input = (((2, 1), 0.5), [(4, 2), (6, 3)]) \n",
    "format_result(my_input)\n",
    "```\n",
    "Output: (C1,..., Ck, A) each of A,C1,...,Ck is a point of form (Ax, Ay)\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "``` python\n",
    "((4, 2), (6, 3), (2, 1))\n",
    "```\n",
    "\n",
    "<font color=\"red\">**Hint : **</font> The above example is given just to provide the input and output format. This function is called a different way in the spark exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "format_result",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def format_result(x):\n",
    "    x[1].append(x[0][0])\n",
    "    return tuple(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75c0eeb264968b58",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def to_sorted_points(x):\n",
    "    \"\"\"\n",
    "    Sorts and returns a tuple of points for further processing.\n",
    "    \"\"\"\n",
    "    return tuple(sorted(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c6a770a06a0457a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Here are some functions that you will implement. You should follow the function definitions, and use them appropriately elsewhere in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7c137c5428c4ccb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1: to_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-505e6159d7dfd5e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Example\n",
    "The function <font color=\"blue\">to_tuple</font> converts each point of form 'Ax Ay' into a point of form (Ax, Ay) for further processing.\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Code</font>**\n",
    "``` python\n",
    "my_input = '2 3'\n",
    "to_tuple(my_input)\n",
    "```\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "``` python\n",
    "(2, 3)\n",
    "```\n",
    "\n",
    "<font color=\"red\">**Hint : **</font> The above example is given just to provide the input and output format. This function is called a different way in the spark exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-856edb9202d856c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "to_tuple",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Insert answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "\n",
    "# Input is a string of 2 integer characters\n",
    "#\n",
    "# Function splits the string into a list, converts each list element to an interger, and returns a tuple of those \n",
    "# integers.\n",
    "\n",
    "def to_tuple(x):\n",
    "    return tuple([int(i) for i in x.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba456fcfa242bc50",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "to_tuple_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(to_tuple('1 1')) == tuple, \"Incorrect type: Element returned is not a tuple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "to_tuple_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(to_tuple('1 1')[0])==int and type(to_tuple('1 1')[1])==int, \"Incorrect element type: Element returned is not an integer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "to_tuple_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert to_tuple('1 1') == (1,1), \"Incorrect Return Value: Value obtained does not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b9993388745a787",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 2: non_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-91b1ac852bcb7b5e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "The function <font color=\"blue\">non_duplicates</font> checks if a set of points contains duplicates or not.\n",
    "\n",
    "Input: Pair (A,B) where A and B are of form (Ax, Ay) and (Bx, By) respectively.\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Code</font>**\n",
    "``` python\n",
    "my_input = ((0,0),(1,2))\n",
    "non_duplicates(my_input)\n",
    "```\n",
    "\n",
    "Output: Returns True if A != B, False otherwise.\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "``` python\n",
    "True\n",
    "```\n",
    "\n",
    "<font color=\"red\">**Hint : **</font> The above example is given just to provide the input and output format. This function may be used to \"filter\" out duplicates inside the get_cartesian() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bf1e0e15843bb79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "non_duplicates",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "\n",
    "# Input is 2 tuples that each are integer pairs that represent cartisian points\n",
    "\n",
    "# Function returns True if the 2 points are not equal\n",
    "\n",
    "def non_duplicates(x):\n",
    "    \"\"\" \n",
    "    Use this function inside the get_cartesian() function to 'filter' out pairs with duplicate points\n",
    "    \"\"\"\n",
    "    if len(x) == len(set(x)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b4079a5fac4a570",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "non_duplicate_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(non_duplicates(((0,0),(1,2)))) == bool, \"Incorrect Return type: Function should return a boolean value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "non_duplicate_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert non_duplicates(((0,0),(1,2))) == True, \"No duplicates are present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "non_duplicate_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert non_duplicates(((0,0),(0,0))) == False, \"Duplicates exist: (0,0)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f28c29e111d2c46b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 3: get_cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7d02f2639ac31d4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "The function <font color=\"blue\">get_cartesian</font> does a cartesian product of an RDD with itself and returns an RDD with <b>DISTINCT</b> pairs of points.\n",
    "\n",
    "Input: An RDD containing the given list of points\n",
    "\n",
    "Output: An RDD containing The cartesian product of the RDD with itself\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Code</font>**\n",
    "``` python\n",
    "test_rdd = sc.parallelize([(1,0), (2,0), (3,0)])\n",
    "get_cartesian(test_rdd).collect()\n",
    "```\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "``` python\n",
    "[((1, 0), (2, 0)), ((1, 0), (3, 0)), ((2, 0), (1, 0)), ((2, 0), (3, 0)), ((3, 0), (1, 0)), ((3, 0), (2, 0))]\n",
    "```\n",
    "\n",
    "Refer:  http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=cartesian#pyspark.RDD.cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-039b50aeadf71899",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_cartesian",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def get_cartesian(rdd):\n",
    "    \n",
    "# Input is an RDD that contains multiple \n",
    "    \n",
    "    return rdd.cartesian(rdd).filter(lambda x: non_duplicates(x) == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-055907b27f0b6fe1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "get_cartesian_test1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_rdd = sc.parallelize([(1,0), (2,0), (3,0)])\n",
    "\n",
    "l = [((1, 0), (2, 0)), ((1, 0), (3, 0)), ((2, 0), (1, 0)), ((2, 0), (3, 0)), ((3, 0), (1, 0)), ((3, 0), (2, 0))]\n",
    "\n",
    "assert isinstance(get_cartesian(test_rdd), RDD) == True, \"Incorrect Return type: Function should return an RDD\"\n",
    "assert set(get_cartesian(test_rdd).collect()) == set(l), \"Incorrect Return Value: Value obtained does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "get_cartesian_test2",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "get_cartesian_test4",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eaec56ed0650e4d5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 4: find_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b4545887393d7f2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "The function <font color=\"blue\">find_slope</font> computes slope between points A and B and returns it in the format specified below.\n",
    "\n",
    "Input: Pair (A,B) where A and B are of form (Ax, Ay) and (Bx, By) respectively. \n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Code</font>**\n",
    "``` python\n",
    "my_input = ((1,2),(3,4))\n",
    "find_slope(my_input)\n",
    "```\n",
    "\n",
    "Output: Pair ((A,slope), B) where A and B have the same definition as input and slope refers to the slope of the line segment connecting point A and B.\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "``` python\n",
    "(((1, 2), 1.0), (3, 4))\n",
    "```\n",
    "<font color=\"brown\">**Note: **</font> If Ax == Bx, use slope as \"inf\".\n",
    "\n",
    "<font color=\"red\">**Hint : **</font> The above example is given just to provide the input and output format. This function is called a different way in the spark exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f2de1e073e2055b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "find_slope",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell\n",
    "\n",
    "def find_slope(x):\n",
    "    \n",
    "    if x[0][0] == x[1][0]:\n",
    "        slope = 'inf'\n",
    "    else:\n",
    "        slope = (x[1][1] - x[0][1])/(x[1][0] - x[0][0])\n",
    "    \n",
    "    return tuple(((x[0], slope), x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d904383c1b13721",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_slope1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(find_slope(((1,2),(3,4)))) == tuple, \"Function must return a tuple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_slope2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert find_slope(((1,2),(-7,-2)))[0][1] == 0.5, \"Slope value should be 0.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_slope3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert find_slope(((1,2),(3,4))) == (((1,2),1),(3,4)), \"Incorrect return value: Value obtained does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_slope4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert find_slope(((1,2),(1,5))) == (((1,2),\"inf\"),(1,5)), \"Incorrect return value: Value obtained must have slope 'inf'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_slope5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert find_slope(((1,2),(2,5))) == (((1,2),3),(2,5)), \"Incorrect return value: Value obtained does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_slope_hidden1",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_slope_hidden2",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_slope_hidden3",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1b7f405e6f29b5b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 5: find_collinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-914ae1e774319691",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "The function <font color=\"blue\">find_collinear</font> finds the set of collinear points.\n",
    "\n",
    "Input: An RDD (which is the output of the get_cartesian() function. \n",
    "\n",
    "Output: An RDD containing the list of collinear points formatted according to the <font color=\"blue\">format_result</font> function.\n",
    "\n",
    "Approach:\n",
    "1. Find the slope of the line between all pairs of points A = (Ax, Ay) and B = (Bx, By).\n",
    "2. For each (A, B), find all points C = ((C1x, C1y), (C2x, C2y), ... (Cnx, Cny)) \n",
    "   where slope of (A,B) = slope of (A, Ci).\n",
    "3. Return (A, B, Ck) where Ck = all points of C which satisfy the condition 1.\n",
    "\n",
    "The assert statement unit tests for this function will help you with this.\n",
    "<font color=\"red\">**Hint : **</font>   You should use the above helper functions in conjunction with Spark RDD API (refer http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD)\n",
    "            Finally, use helper function format_result() appropriately from inside this function after you have implemented the above operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a31ef2dcb4560fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "find_collinear",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_collinear(rdd):\n",
    "    \n",
    "    # Process to find sets of collinear points:\n",
    "        # Find the slope of the line of all point pairs\n",
    "        # Group points by slope\n",
    "        # Strip the slope from each set and sort points\n",
    "        # Eliminate all sets of size 2\n",
    "        # Return unique sets of points\n",
    "    \n",
    "    return rdd.map(lambda x: find_slope((x[0],x[1]))).groupByKey().mapValues(list).map(lambda x:to_sorted_points(format_result(x))).filter(lambda x: len(x) > 2).distinct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffb01083af217fe4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-adee31e44b3bf643",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def verify_collinear_sets(collinearpointsRDD, testlist):\n",
    "    collinearpoints = [tuple(sorted(x)) for x in list(set(collinearpointsRDD.collect()))]\n",
    "    testlist = [tuple(sorted(x)) for x in list(set(testlist))]\n",
    "    return set(collinearpoints) == set(testlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_collinear1",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_rdd = sc.parallelize([((4, 2), (2, 1)), ((4, 2), (-3, 4)), ((4, 2), (6, 3)), ((2, 1), (4, 2)), ((2, 1), (-3, 4)), ((2, 1), (6, 3)), ((-3, 4), (4, 2)), ((-3, 4), (2, 1)), ((-3, 4), (6, 3)), ((6, 3), (4, 2)), ((6, 3), (2, 1)), ((6, 3), (-3, 4))])\n",
    "assert isinstance(find_collinear(test_rdd), RDD) == True, \"Incorrect return type: Function must return RDD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_collinear2",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 39, host.docker.internal, executor driver): java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:131)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:103)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:131)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:103)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-913834fd7f36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mverify_collinear_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_collinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_rdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Incorrect return value: Value obtained does not match\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-6fba292a1c0f>\u001b[0m in \u001b[0;36mverify_collinear_sets\u001b[1;34m(collinearpointsRDD, testlist)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mverify_collinear_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollinearpointsRDD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcollinearpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollinearpointsRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtestlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollinearpoints\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 39, host.docker.internal, executor driver): java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:131)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:103)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:165)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:131)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:103)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "assert verify_collinear_sets(find_collinear(test_rdd), [((2, 1), (4, 2), (6, 3))]), \"Incorrect return value: Value obtained does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_collinear_hidden",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbc9a77136a57c34",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Unit Tests II : Using the output of get_cartesian(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_collinear3",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_rdd = sc.parallelize([(4, -2), (2, -1), (-3,4), (6,3), (-9,4), (6, -3), (8,-4), (6,9)])\n",
    "test_rdd = get_cartesian(test_rdd)\n",
    "assert verify_collinear_sets(find_collinear(test_rdd), [((6, -3), (6, 3), (6, 9)), ((2, -1), (4, -2), (6, -3), (8, -4))]), \"Incorrect return value: You have not implemented the find_collinear function in Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "find_collinear_hidden2",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a15b922949251b16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise 6: The build_collinear_set function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a15b922949251b15",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Example\n",
    "Using the above functions that you have written along with pyspark functions, write the **build_collinear_set** function and returns an RDD containing the set of collinear points.\n",
    "\n",
    "Input: RDD containing the given set of points\n",
    "\n",
    "Output: RDD containing the set of collinear points\n",
    "\n",
    "<font color=\"red\">**Hint : **</font> Remember that the input RDD consists of a set of strings. Remember to pre-process them using the to_tuple function before performing other operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e65a588686d085c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "Collinear-main",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_collinear_set(rdd):\n",
    "    \n",
    "    # Convert input of points in text format to tuples of integers\n",
    "    rdd.map(lambda x: to_tuple(x))\n",
    "    \n",
    "    # Get all pairs of points\n",
    "    rdd = get_cartesian(rdd)\n",
    "    \n",
    "    # Generate sets of collinear points\n",
    "    rdd = find_collinear(rdd)\n",
    "    \n",
    "    # Sorting each of your returned sets of collinear points. This is for grading purposes. \n",
    "    # YOU MUST NOT CHANGE THIS.\n",
    "    rdd = rdd.map(to_sorted_points)\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d08e0ec753a680b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Collinear-type-test",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_rdd = sc.parallelize(['4 -2', '2 -1', '-3 4', '6 3', '-9 4', '6 -3', '8 -4', '6 9'])\n",
    "assert isinstance(build_collinear_set(test_rdd), RDD) == True, \"build_collinear_set should return an RDD.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cc72c2cc0fb5d36b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### The process function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a1c492646e86a45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "Main-function",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def process(filename):\n",
    "    \"\"\"\n",
    "    This is the process function used for finding collinear points using inputs from different files\n",
    "    Input: Name of the test file\n",
    "    Output: Set of collinear points\n",
    "    \"\"\"\n",
    "    # Load the data file into an RDD\n",
    "    rdd = sc.textFile(filename).map(lambda x: to_tuple(x))\n",
    "    \n",
    "    # Find all sets of 3 or more points that form a line\n",
    "    rdd = build_collinear_set(rdd)\n",
    "    \n",
    "    # Collecting the collinear points RDD in a set to remove duplicate sets of collinear points. This is for grading purposes. You may ignore this.\n",
    "    res = set(rdd.collect())\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-614b27038af0cd2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Unit Tests: Testing the build_collinear_set function using the process function\n",
    "NOTE: You may assume that input files do not have duplicate points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Collinear_soln_visible1",
     "locked": false,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert process(\"../resource/asnlib/public/data.txt\") == {((-2, -2), (1, 1), (2, 2), (3, 3)), ((0, 1), (3, 4), (5, 6)), ((0, -3), (0, 1), (0, 5))}, \"Your implementation of build_collinear_set is not correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Collinear_soln_visible2",
     "locked": false,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o263.cartesian.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/qcsbo/Documents/GitHub/Micro-Masters/Spark/Week 1 & 2/resource/asnlib/public/data50.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.CartesianRDD.<init>(CartesianRDD.scala:56)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$cartesian$1(RDD.scala:715)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.cartesian(RDD.scala:715)\r\n\tat org.apache.spark.api.java.JavaRDDLike.cartesian(JavaRDDLike.scala:233)\r\n\tat org.apache.spark.api.java.JavaRDDLike.cartesian$(JavaRDDLike.scala:232)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.cartesian(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-2100831e3b12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m assert process(\"../resource/asnlib/public/data50.txt\") == {((3, 6), (7, 4), (9, 3)), ((1, 6), (3, 6), (4, 6), (7, 6)), \n\u001b[0m\u001b[0;32m      2\u001b[0m                                  \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                  \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                  \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-73249d0a1a39>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Find all sets of 3 or more points that form a line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_collinear_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Collecting the collinear points RDD in a set to remove duplicate sets of collinear points. This is for grading purposes. You may ignore this.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-7b7154b9fb14>\u001b[0m in \u001b[0;36mbuild_collinear_set\u001b[1;34m(rdd)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Get all pairs of points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cartesian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Generate sets of collinear points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-c2a20f9154c3>\u001b[0m in \u001b[0;36mget_cartesian\u001b[1;34m(rdd)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Input is an RDD that contains multiple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartesian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnon_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcartesian\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    798\u001b[0m         deserializer = CartesianDeserializer(self._jrdd_deserializer,\n\u001b[0;32m    799\u001b[0m                                              other._jrdd_deserializer)\n\u001b[1;32m--> 800\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartesian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mportable_hash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o263.cartesian.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/qcsbo/Documents/GitHub/Micro-Masters/Spark/Week 1 & 2/resource/asnlib/public/data50.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.CartesianRDD.<init>(CartesianRDD.scala:56)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$cartesian$1(RDD.scala:715)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.cartesian(RDD.scala:715)\r\n\tat org.apache.spark.api.java.JavaRDDLike.cartesian(JavaRDDLike.scala:233)\r\n\tat org.apache.spark.api.java.JavaRDDLike.cartesian$(JavaRDDLike.scala:232)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.cartesian(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "assert process(\"../resource/asnlib/public/data50.txt\") == {((3, 6), (7, 4), (9, 3)), ((1, 6), (3, 6), (4, 6), (7, 6)), \n",
    "                                 ((0, 2), (3, 1), (6, 0)), ((1, 0), (2, 0), (5, 0), (6, 0)), \n",
    "                                 ((1, 3), (3, 6), (5, 9)), ((0, 8), (4, 6), (6, 5)), \n",
    "                                 ((6, 0), (6, 1), (6, 5), (6, 9)), \n",
    "                                 ((7, 2), (7, 3), (7, 4), (7, 6), (7, 8)), ((3, 1), (3, 3), (3, 6)), \n",
    "                                 ((0, 2), (1, 2), (5, 2), (7, 2)), ((0, 3), (2, 5), (3, 6), (6, 9)), \n",
    "                                 ((0, 2), (1, 3), (2, 4), (4, 6), (5, 7)), ((1, 2), (4, 3), (7, 4)), \n",
    "                                 ((0, 3), (4, 6), (8, 9)), ((9, 3), (9, 4), (9, 5)), ((2, 5), (5, 7), (8, 9)), \n",
    "                                 ((0, 5), (2, 4), (4, 3), (8, 1)), ((0, 8), (1, 6), (2, 4)), \n",
    "                                 ((3, 6), (5, 2), (6, 0)), ((5, 9), (6, 9), (8, 9)), \n",
    "                                 ((0, 8), (1, 8), (7, 8)), ((0, 4), (1, 3), (3, 1)), ((5, 9), (7, 6), (9, 3)), \n",
    "                                 ((1, 2), (2, 4), (3, 6)), ((0, 7), (1, 5), (3, 1)), \n",
    "                                 ((1, 5), (2, 4), (3, 3), (6, 0)), ((0, 2), (3, 3), (9, 5)), \n",
    "                                 ((0, 7), (1, 6), (2, 5), (4, 3), (5, 2), (6, 1)), \n",
    "                                 ((0, 4), (1, 5), (5, 9)), ((1, 5), (3, 6), (5, 7), (7, 8)), \n",
    "                                 ((1, 6), (3, 3), (5, 0)), ((3, 6), (4, 3), (5, 0)), \n",
    "                                 ((1, 2), (4, 5), (7, 8), (8, 9)), ((0, 2), (1, 1), (2, 0)), \n",
    "                                 ((3, 3), (4, 5), (5, 7), (6, 9)), ((0, 2), (0, 3), (0, 4), (0, 5), (0, 7), (0, 8)), \n",
    "                                 ((2, 0), (4, 3), (8, 9)), ((5, 7), (6, 5), (7, 3), (8, 1)), ((5, 0), (7, 6), (8, 9)), \n",
    "                                 ((5, 0), (6, 1), (7, 2), (9, 4)), ((0, 4), (1, 2), (2, 0)), \n",
    "                                 ((1, 1), (3, 1), (6, 1), (8, 1)), ((5, 7), (7, 6), (9, 5)), ((1, 1), (7, 4), (9, 5)), \n",
    "                                 ((0, 4), (2, 4), (7, 4), (9, 4)), ((1, 0), (3, 1), (5, 2), (7, 3), (9, 4)), \n",
    "                                 ((2, 0), (3, 3), (4, 6), (5, 9)), ((4, 3), (4, 5), (4, 6)), \n",
    "                                 ((1, 0), (4, 3), (6, 5), (7, 6)), ((0, 3), (2, 4), (4, 5)), \n",
    "                                 ((1, 6), (4, 5), (7, 4)), ((1, 0), (1, 1), (1, 2), (1, 3), (1, 5), (1, 6), (1, 8)), \n",
    "                                 ((0, 3), (1, 3), (3, 3), (4, 3), (7, 3), (9, 3)), ((0, 4), (2, 5), (4, 6)), \n",
    "                                 ((0, 7), (3, 6), (6, 5), (9, 4)), ((1, 8), (4, 6), (7, 4)), \n",
    "                                 ((0, 5), (3, 3), (6, 1)), ((1, 8), (3, 6), (4, 5), (7, 2), (8, 1)), \n",
    "                                 ((1, 2), (3, 1), (5, 0)), ((1, 1), (5, 2), (9, 3)), \n",
    "                                 ((5, 0), (5, 2), (5, 7), (5, 9)), ((0, 5), (1, 5), (2, 5), (4, 5), (6, 5), (9, 5)), \n",
    "                                 ((3, 1), (4, 5), (5, 9)), ((2, 0), (2, 4), (2, 5)), ((5, 2), (6, 5), (7, 8))}, \"Your implementation of build_collinear_set is not correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Collinear_soln_hidden1",
     "locked": false,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Collinear_soln_hidden2",
     "locked": false,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Collinear_soln_hidden3",
     "locked": false,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Collinear_soln_hidden4",
     "locked": false,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##Hidden test cases here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": [],
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "573px",
    "left": "55px",
    "top": "111px",
    "width": "269px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
